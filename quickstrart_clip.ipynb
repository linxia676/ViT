{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "class CLIPModelWrapper:\n",
    "    def __init__(self, model_name=\"openai/clip-vit-base-patch32\", device=None):\n",
    "        \"\"\"\n",
    "        初始化 CLIP 模型和处理器\n",
    "        :param model_name: 使用的预训练模型名称\n",
    "        :param device: 使用的设备（'cuda' 或 'cpu'）\n",
    "        \"\"\"\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = CLIPModel.from_pretrained(model_name).to(self.device)\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "    def encode_text(self, texts):\n",
    "        \"\"\"\n",
    "        对输入文本进行编码\n",
    "        :param texts: 字符串列表\n",
    "        :return: 文本特征张量\n",
    "        \"\"\"\n",
    "        inputs = self.processor(text=texts, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            text_features = self.model.get_text_features(**inputs)\n",
    "        return text_features / text_features.norm(dim=-1, keepdim=True)  # 归一化\n",
    "\n",
    "    def encode_image(self, images):\n",
    "        \"\"\"\n",
    "        对输入图像进行编码\n",
    "        :param images: PIL Image 对象或 PIL 图像列表\n",
    "        :return: 图像特征张量\n",
    "        \"\"\"\n",
    "        inputs = self.processor(images=images, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.get_image_features(**inputs)\n",
    "        return image_features / image_features.norm(dim=-1, keepdim=True)  # 归一化\n",
    "\n",
    "    def calculate_similarity(self, image_features, text_features):\n",
    "        \"\"\"\n",
    "        计算图像和文本特征之间的余弦相似度\n",
    "        :param image_features: 图像特征张量\n",
    "        :param text_features: 文本特征张量\n",
    "        :return: 相似度张量\n",
    "        \"\"\"\n",
    "        similarity = torch.nn.functional.cosine_similarity(image_features, text_features)\n",
    "        return (100.0 * similarity).softmax(dim=-1)\n",
    "\n",
    "    def classify_image(self, image, categories):\n",
    "        \"\"\"\n",
    "        对图像进行分类\n",
    "        :param image: 单张 PIL 图像\n",
    "        :param categories: 类别描述列表\n",
    "        :return: 最可能的类别和相似度\n",
    "        \"\"\"\n",
    "        text_features = self.encode_text([f\"A photo of {category}\" for category in categories])\n",
    "        image_features = self.encode_image([image])\n",
    "        similarities = self.calculate_similarity(image_features, text_features)\n",
    "        best_idx = similarities.argmax()\n",
    "        return categories[best_idx], similarities[best_idx].item()\n",
    "\n",
    "    def batch_classify_images(self, images, categories):\n",
    "        \"\"\"\n",
    "        对多张图像进行分类\n",
    "        :param images: PIL 图像列表\n",
    "        :param categories: 类别描述列表\n",
    "        :return: 每张图像的分类结果\n",
    "        \"\"\"\n",
    "        text_features = self.encode_text([f\"A photo of {category}\" for category in categories])\n",
    "        image_features = self.encode_image(images)\n",
    "        results = []\n",
    "        for i, img_feat in enumerate(image_features):\n",
    "            similarities = self.calculate_similarity(img_feat.unsqueeze(0), text_features)\n",
    "            best_idx = similarities.argmax()\n",
    "            results.append((categories[best_idx], similarities[best_idx].item()))\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a cat', 0.9960488677024841)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = r'data\\test_img\\cat.jpg'\n",
    "\n",
    "input_img = Image.open(img_path)\n",
    "input_text = ['a cat', 'a dog', 'a bird']\n",
    "\n",
    "clip_model = CLIPModelWrapper()\n",
    "clip_model.classify_image(input_img, input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
